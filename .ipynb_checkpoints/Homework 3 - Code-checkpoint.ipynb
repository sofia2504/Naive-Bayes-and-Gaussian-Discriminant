{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f528c8af",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 24px;\">Binary Classification on Text Data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ca92f",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px;\">Part 1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04a1300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Sofia\n",
      "[nltk_data]     Beyerlein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "\n",
    "import torch.autograd as tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa22778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD THE DATA\n",
    "filepath_train = (r\"C:\\Users\\Sofia Beyerlein\\Desktop\\Cornell Graduate\\Applied Machine Learning\\hw2\\nlp-getting-started\\train.csv\")\n",
    "filepath_test = (r'C:\\Users\\Sofia Beyerlein\\Desktop\\Cornell Graduate\\Applied Machine Learning\\hw2\\nlp-getting-started\\test.csv')\n",
    "\n",
    "train = pd.read_csv(filepath_train)\n",
    "test = pd.read_csv(filepath_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cffe0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE DATA\n",
    "#70% -> 5329/7613 and 30% -> 2284\n",
    "training_set = train.sample(frac=0.7)\n",
    "dev_set = train.drop(training_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29f8530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heavy rain cause flash flooding street manitou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>haha south tampa getting flooded hah wait seco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bago myanmar arrived bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>summer lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7572</th>\n",
       "      <td>10823</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>get wrecked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591</th>\n",
       "      <td>10846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heat wave warning aa ayyo dei plan visit frien...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>10850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nw flash flood warning continued shelby county...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7602</th>\n",
       "      <td>10860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>siren went wasnt forney tornado warning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant crane holding bridge collapse nearby...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2284 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword       location  \\\n",
       "2         5      NaN            NaN   \n",
       "6        10      NaN            NaN   \n",
       "11       17      NaN            NaN   \n",
       "13       19      NaN            NaN   \n",
       "17       25      NaN            NaN   \n",
       "...     ...      ...            ...   \n",
       "7572  10823  wrecked  Manhattan, NY   \n",
       "7591  10846      NaN            NaN   \n",
       "7595  10850      NaN            NaN   \n",
       "7602  10860      NaN            NaN   \n",
       "7608  10869      NaN            NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "2     resident asked shelter place notified officer ...       1  \n",
       "6     heavy rain cause flash flooding street manitou...       1  \n",
       "11    haha south tampa getting flooded hah wait seco...       1  \n",
       "13                            bago myanmar arrived bago       1  \n",
       "17                                        summer lovely       0  \n",
       "...                                                 ...     ...  \n",
       "7572                                        get wrecked       1  \n",
       "7591  heat wave warning aa ayyo dei plan visit frien...       1  \n",
       "7595  nw flash flood warning continued shelby county...       1  \n",
       "7602            siren went wasnt forney tornado warning       1  \n",
       "7608  two giant crane holding bridge collapse nearby...       1  \n",
       "\n",
       "[2284 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_data(df):\n",
    "    words_to_remove = {'the', 'and', 'or'}\n",
    "    #lowercase\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "    #remove @ and urls\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'@\\S+', '', x))\n",
    "    #remove # and hashtags\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'#\\S+', '', x))\n",
    "    #strip punctuation\n",
    "    df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    #strip the and or\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(word for word in x.split() if word not in words_to_remove))\n",
    "    #lemmatize\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def lemmatize_text(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    df['text'] = df['text'].apply(lemmatize_text)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "        \n",
    "preprocess_data(training_set)\n",
    "preprocess_data(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49b4245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "vectorizer = CountVectorizer(binary=True, min_df=M)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(training_set['text'])\n",
    "dev_vectors = vectorizer.transform(dev_set['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1933234",
   "metadata": {},
   "source": [
    "<span style = \"font-size: 18px;\">Part 1 a: Bernoulli Naive Bayes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b3b9724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7589743589743588\n"
     ]
    }
   ],
   "source": [
    "# Define functions for the Bernoulli Naive Bayes Classifier with Laplace Smoothing\n",
    "#\n",
    "def calculate_class_priors(y):\n",
    "    num_docs = len(y)\n",
    "    class_priors = np.bincount(y) / num_docs\n",
    "    return class_priors\n",
    "\n",
    "def calculate_feature_probs(X, y, alpha=1):\n",
    "    num_docs, num_features = X.shape\n",
    "    feature_probs = np.zeros((2, num_features))\n",
    "    \n",
    "    for k in range(2):\n",
    "        class_docs = X[y == k]\n",
    "        feature_probs[k] = (class_docs.sum(axis=0) + alpha) / (class_docs.shape[0] + 2 * alpha)\n",
    "    \n",
    "    return feature_probs\n",
    "\n",
    "def predict_log_proba(X, class_priors, feature_probs):\n",
    "    num_docs, num_features = X.shape\n",
    "    log_probs = np.zeros((num_docs, 2))\n",
    "    \n",
    "    for k in range(2):\n",
    "        # Log probability for class k\n",
    "        log_prob_k = np.log(class_priors[k])\n",
    "        log_prob_x_given_k = X @ np.log(feature_probs[k]) + (1 - X) @ np.log(1 - feature_probs[k])\n",
    "        log_probs[:, k] = log_prob_k + log_prob_x_given_k\n",
    "        \n",
    "    return log_probs\n",
    "\n",
    "def predict(X, class_priors, feature_probs):\n",
    "    log_probs = predict_log_proba(X, class_priors, feature_probs)\n",
    "    return np.argmax(log_probs, axis=1)\n",
    "\n",
    "# Calculate class priors\n",
    "class_priors = calculate_class_priors(training_set['target'].values)\n",
    "\n",
    "# Calculate feature probabilities with Laplace smoothing\n",
    "feature_probs = calculate_feature_probs(train_vectors.toarray(), training_set['target'].values, alpha=1)\n",
    "\n",
    "# Predict on the development set\n",
    "dev_predictions = predict(dev_vectors.toarray(), class_priors, feature_probs)\n",
    "\n",
    "# Calculate the F1 score for the development set\n",
    "f1 = f1_score(dev_set['target'], dev_predictions)\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
